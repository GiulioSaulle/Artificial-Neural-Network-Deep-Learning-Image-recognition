{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1jqF0ynJli"
      },
      "source": [
        "## HOMEWORK 1 ##\n",
        "Custom CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoaLQpvChLpb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive/My Drive/[2023-2024] AN2DL/Homework 1\n",
        "%cd /gdrive/My Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D_S1JfaW8bIN"
      },
      "outputs": [],
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7TRtQ5GupYFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009bdc5d-9a2e-4477-ebb1-ba892dd791f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GOZp5M5QM88W"
      },
      "outputs": [],
      "source": [
        "# Import other libraries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYr7QMSq3W0n"
      },
      "source": [
        "## Load Data + Display images ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRClc_FznEtI"
      },
      "outputs": [],
      "source": [
        "# Conditional check for unzipping\n",
        "unzip = False\n",
        "\n",
        "# Unzip the 'animals.zip' file if the 'unzip' flag is True\n",
        "if unzip:\n",
        "    !unzip public_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s0BPSBz-zigx"
      },
      "outputs": [],
      "source": [
        "# Reading the .npz file\n",
        "data = np.load('public_data.npz', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQW5O2Vk5B85"
      },
      "outputs": [],
      "source": [
        "# Getting the images and labels\n",
        "data_array = data['data']\n",
        "labels_array = data['labels']\n",
        "\n",
        "# Check the shapes of the array\n",
        "print(\"Data array shape: \", data_array.shape)\n",
        "print(\"Data array shape: \", labels_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7r9tu8dm33Au"
      },
      "outputs": [],
      "source": [
        "# Getting images in a np.array\n",
        "images = []\n",
        "\n",
        "for img in data_array:\n",
        "  img=(img).astype(np.float32)\n",
        "  images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaxD9ASi4Oow"
      },
      "outputs": [],
      "source": [
        "# Number of images to display\n",
        "num_img = 10\n",
        "\n",
        "# Create subplots for displaying leaves\n",
        "fig, axes = plt.subplots(2, num_img//2, figsize=(8, 4))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i%2, i%num_img//2]\n",
        "    ax.imshow(np.clip(images[i]/255, 0, 255))  # Display clipped item images\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8B4PT1F6uID"
      },
      "source": [
        "## Creating labels and sets ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uTArdC7F6wZA"
      },
      "outputs": [],
      "source": [
        "y = []\n",
        "count_h = 0\n",
        "# Create an array with zeros and ones based on the tag\n",
        "for i in range(len(images)):\n",
        "    if labels_array[i] == 'healthy':\n",
        "        count_h = count_h + 1\n",
        "        y.append(0)\n",
        "    else:\n",
        "        y.append(1)\n",
        "\n",
        "# Convert labesl to one-hot encoding format\n",
        "y = tfk.utils.to_categorical(y,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNkgPiPG-tBX"
      },
      "outputs": [],
      "source": [
        "# Count the number of healthy labels\n",
        "print(\"Total number of labels: \" + str(len(images)))\n",
        "print(\"Number of healthy labels: \" + str(count_h))\n",
        "print(\"Number of unhealthy labels: \" + str(len(images) - count_h))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuHCsHP_9yY4"
      },
      "outputs": [],
      "source": [
        "# Creating the sets\n",
        "X = np.array(images)\n",
        "\n",
        "# Split data into train, validation and test set\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=0.1, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5BHZke1DIZO"
      },
      "source": [
        "## Adding noise ONLY to the train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HpcDKr6H-mJE"
      },
      "outputs": [],
      "source": [
        "# Adding noise\n",
        "X_train_noise = []\n",
        "\n",
        "for image in X_train:\n",
        "    # Adding noise to the image\n",
        "    VARIABILITY = 35\n",
        "    deviation = VARIABILITY * random.random()\n",
        "    noise = np.random.normal(0, deviation, image.shape)\n",
        "    image += noise\n",
        "    np.clip(image, 0., 255., out=image)\n",
        "    X_train_noise.append(image)\n",
        "\n",
        "X_train = np.array(X_train_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89j6EoN_-mJF"
      },
      "outputs": [],
      "source": [
        "# Display leaves with noise\n",
        "# Number of images to display\n",
        "num_img = 10\n",
        "\n",
        "# Create subplots for displaying leaves\n",
        "fig, axes = plt.subplots(2, num_img//2, figsize=(8, 4))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i%2, i%num_img//2]\n",
        "    ax.imshow(np.clip(X_train[i]/255, 0, 255))  # Display clipped item images\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-2C5sZpDz67"
      },
      "source": [
        "## Creating the model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs7BTXJlOvvz"
      },
      "outputs": [],
      "source": [
        "# Defining input and output shapes\n",
        "input_shape = X_train.shape[1:]\n",
        "output_shape = y_train.shape[1:]\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(input_shape)\n",
        "print(output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SVw53LU-DzIi"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape=input_shape, output_shape=output_shape):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Data augmentation\n",
        "    data_aug = tf.keras.Sequential([\n",
        "        tfkl.RandomFlip('horizontal_and_vertical'),\n",
        "        tfkl.RandomRotation(0.5),\n",
        "        tfkl.RandomTranslation(0.2, 0.2),\n",
        "        tfkl.RandomZoom(0.2),\n",
        "        tfkl.RandomBrightness(0.2, value_range=(0,255)),\n",
        "        tfkl.RandomContrast(0.2),\n",
        "    ], name='data_aug')\n",
        "\n",
        "    # Build the input layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    x = data_aug(input_layer)\n",
        "\n",
        "    # 96x96x3\n",
        "    x = tfkl.Conv2D(filters=55, kernel_size=3, activation='relu', padding='valid', name='conv01')(x)\n",
        "    x = tfkl.Conv2D(filters=70, kernel_size=3, activation='relu', padding='valid', name='conv02')(x)\n",
        "    x = tfkl.Conv2D(filters=85, kernel_size=3, activation='relu', padding='valid', name='conv03')(x)\n",
        "    x = tfkl.Conv2D(filters=100, kernel_size=3, activation='relu', padding='valid', name='conv04')(x)\n",
        "    x = tfkl.MaxPooling2D(pool_size =(2,2), name='mp0')(x)\n",
        "    # 44x44x80\n",
        "\n",
        "    x = tfkl.Normalization()(x)\n",
        "\n",
        "    # 44x44x80\n",
        "    x = tfkl.Conv2D(filters=155, kernel_size=3, activation='relu', padding='valid', name='conv11')(x)\n",
        "    x = tfkl.Conv2D(filters=170, kernel_size=3, activation='relu', padding='valid', name='conv12')(x)\n",
        "    x = tfkl.Conv2D(filters=185, kernel_size=3, activation='relu', padding='valid', name='conv13')(x)\n",
        "    x = tfkl.Conv2D(filters=200, kernel_size=3, activation='relu', padding='valid', name='conv14')(x)\n",
        "    x = tfkl.MaxPooling2D(pool_size =(2,2), name='mp1')(x)\n",
        "    # 18x18x200\n",
        "\n",
        "    x = tfkl.Normalization()(x)\n",
        "\n",
        "    # 18x18x200\n",
        "    conv1_1x1 = tfkl.Conv2D(filters=150, kernel_size=1, activation='relu', padding='same', name='conv1_1x1')(x)\n",
        "    conv2_1x1 = tfkl.Conv2D(filters=50, kernel_size=1, activation='relu', padding='same', name='conv2_1x1')(x)\n",
        "    conv2_3x3 = tfkl.Conv2D(filters=150, kernel_size=1, activation='relu', padding='same', name='conv2_3x3')(conv2_1x1)\n",
        "    conv3_1x1 = tfkl.Conv2D(filters=50, kernel_size=1, activation='relu', padding='same', name='conv3_1x1')(x)\n",
        "    conv3_5x5 = tfkl.Conv2D(filters=150, kernel_size=1, activation='relu', padding='same', name='conv3_5x5')(conv3_1x1)\n",
        "    conv4_1x1 = tfkl.Conv2D(filters=50, kernel_size=1, activation='relu', padding='same', name='conv4_1x1')(x)\n",
        "    conv4_7x7 = tfkl.Conv2D(filters=150, kernel_size=1, activation='relu', padding='same', name='conv4_7x7')(conv4_1x1)\n",
        "    mp5_3x3 = tfkl.MaxPooling2D(pool_size =(3,3), strides=1, padding='same', name='mp5_3x3')(x)\n",
        "    conv5_1x1 = tfkl.Conv2D(filters=150, kernel_size=1, activation='relu', padding='same', name='conv5_1x1')(mp5_3x3)\n",
        "\n",
        "    # Concatenation of the previous layers\n",
        "    x = tfkl.Concatenate(name='Concatenation')([conv1_1x1, conv2_3x3, conv3_5x5, conv4_7x7, conv5_1x1])\n",
        "    # 18x18x750\n",
        "\n",
        "    # GAP\n",
        "    x = tfkl.GlobalAveragePooling2D(name='gap')(x)\n",
        "\n",
        "    # Leaky ReLU after the gap\n",
        "    x = tfkl.LeakyReLU()(x)\n",
        "\n",
        "    # Dense layer + dropout & weight decay series\n",
        "    droput_rate = 1/6;\n",
        "    l2_lambda = 2e-6;\n",
        "\n",
        "    x = tfkl.Dense(units=600, activation='gelu', kernel_initializer=tfk.initializers.HeUniform(seed=seed), kernel_regularizer=tf.keras.regularizers.l2(l2_lambda),name='dense1')(x)\n",
        "    x = tfkl.Dropout(droput_rate, seed=seed)(x)\n",
        "\n",
        "    x = tfkl.Dense(units=325, activation='gelu', kernel_initializer=tfk.initializers.HeUniform(seed=seed), kernel_regularizer=tf.keras.regularizers.l2(l2_lambda),name='dense2')(x)\n",
        "    x = tfkl.Dropout(droput_rate, seed=seed)(x)\n",
        "\n",
        "    x = tfkl.Dense(units=50, activation='gelu', kernel_initializer=tfk.initializers.HeUniform(seed=seed), kernel_regularizer=tf.keras.regularizers.l2(l2_lambda),name='dense3')(x)\n",
        "    x = tfkl.Dropout(droput_rate, seed=seed)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = tfkl.Dense(units=2, activation='softmax',name='Output')(x)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='BuddysCNNV final')\n",
        "\n",
        "    # Compile the model\n",
        "    learning_rate = 1e-3\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(label_smoothing=0.15), optimizer=tfk.optimizers.Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGnI8fz9L-m_"
      },
      "outputs": [],
      "source": [
        "# Print important the summary of the model and it's graph\n",
        "model = build_model()\n",
        "model.summary()\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class weights to balance classes\n",
        "ymax = np.argmax(y_train, axis=1)\n",
        "weights = class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                 classes=np.unique(np.argmax(y_train, axis=1)),\n",
        "                                                 y=ymax)\n",
        "print(weights)\n",
        "\n",
        "class_weights = {0: weights[0], 1: weights[1]}"
      ],
      "metadata": {
        "id": "GhhAg36IDoJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmyC4u1Tq47P"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "history = model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    class_weight=class_weights,\n",
        "    batch_size = 256,\n",
        "    epochs = 750,\n",
        "    validation_data = (X_val, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=60, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.94, min_lr=1e-6)]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOcusMyavSBa"
      },
      "outputs": [],
      "source": [
        "# Printing the best validation accuracy\n",
        "max(history['val_accuracy'])\n",
        "\n",
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "\n",
        "# Plot the training history\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6oTUvlLwomo"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save('BuddysCNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DcuQWep4u1Z"
      },
      "outputs": [],
      "source": [
        "# Delete the model\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apjWisqB4ZlF"
      },
      "outputs": [],
      "source": [
        "# Load the saved LeNet model\n",
        "model = tfk.models.load_model('BuddysCNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNtGMlgA0ry6"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the test set\n",
        "predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "\n",
        "# Display the computed metrics\n",
        "print('Accuracy:', accuracy.round(4))\n",
        "print('Precision:', precision.round(4))\n",
        "print('Recall:', recall.round(4))\n",
        "print('F1:', f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, xticklabels=list(['healthy', 'unhealthy']), yticklabels=list(['healthy', 'unhealthy']), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}